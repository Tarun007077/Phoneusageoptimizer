# Cell 4 — Q-learning training
import random
from collections import defaultdict
Q = defaultdict(lambda: np.zeros(env.action_space.n, dtype=np.float32))

def state_key(s):
    return tuple(np.round(s, 2))

episodes = 3000
alpha = 0.1
gamma = 0.95
epsilon = 0.2

rewards_q = []
for ep in range(episodes):
    state, _ = env.reset()
    key = state_key(state)
    if random.random() < epsilon:
        action = env.action_space.sample()
    else:
        action = int(np.argmax(Q[key]))
    next_state, reward, done, truncated, info = env.step(action)
    next_key = state_key(next_state)
    Q[key][action] += alpha * (reward + gamma * np.max(Q[next_key]) - Q[key][action])
    rewards_q.append(reward)
    if (ep+1) % 500 == 0:
        print(f"Q: episode {ep+1}/{episodes}, recent avg reward: {np.mean(rewards_q[-200:]):.4f}")

plt.plot(pd.Series(rewards_q).rolling(50,min_periods=1).mean())
plt.title('Q-Learning smoothed rewards')
plt.show()
# Cell 5 — Policy Gradient (REINFORCE)
class PolicyNet(nn.Module):
    def __init__(self, state_dim, hidden=64, n_actions=4):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, n_actions),
            nn.Softmax(dim=-1)
        )
    def forward(self, x):
        return self.net(x)

state_dim = X_scaled.shape[1]
policy = PolicyNet(state_dim)
optimizer = optim.Adam(policy.parameters(), lr=1e-3)
gamma = 0.99
episodes = 1500
rewards_pg = []

for ep in range(episodes):
    state, _ = env.reset()
    state_t = torch.tensor(state, dtype=torch.float32)
    probs = policy(state_t)
    dist = torch.distributions.Categorical(probs)
    action = dist.sample()
    logp = dist.log_prob(action)
    next_state, reward, done, truncated, info = env.step(action.item())
    G = reward  # single-step return
    loss = -logp * G
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    rewards_pg.append(reward)
    if (ep+1) % 200 == 0:
        print(f"PG: episode {ep+1}/{episodes}, recent avg reward: {np.mean(rewards_pg[-200:]):.4f}")

plt.plot(pd.Series(rewards_pg).rolling(50,min_periods=1).mean())
plt.title('Policy Gradient smoothed rewards')
plt.show()
# Cell 6 — Contextual Bandit (linear Thompson-style)
n_actions = env.action_space.n
d = X_scaled.shape[1]

A = [np.eye(d) for _ in range(n_actions)]
b = [np.zeros(d) for _ in range(n_actions)]

def select_action_thompson(x):
    samples = []
    for a in range(n_actions):
        A_inv = np.linalg.inv(A[a])
        mu = A_inv.dot(b[a])
        try:
            sample = np.random.multivariate_normal(mu, A_inv)
        except Exception:
            sample = mu
        samples.append(x.dot(sample))
    return int(np.argmax(samples))

rounds = 3000
rewards_bandit = []
for i in range(rounds):
    idx = np.random.randint(0, len(X_scaled))
    x = X_scaled[idx]
    action = select_action_thompson(x)
    env.index = idx
    next_state, reward, done, truncated, info = env.step(action)
    rewards_bandit.append(reward)
    # update
    A[action] += np.outer(x,x)
    b[action] += reward * x
    if (i+1) % 500 == 0:
        print(f"Bandit round {i+1}/{rounds}, recent avg reward: {np.mean(rewards_bandit[-200:]):.4f}")

plt.plot(pd.Series(rewards_bandit).rolling(50,min_periods=1).mean())
plt.title('Bandit smoothed rewards')
plt.show()
# Cell 7 — Evaluation utilities
def evaluate_policy_q(Q_table, env_obj, n_samples=300):
    idxs = np.random.choice(len(env_obj.df), size=n_samples, replace=False)
    metrics = {'unproductive': [], 'satisfaction': [], 'autonomy_penalty': []}
    for idx in idxs:
        env_obj.index = int(idx)
        state, _ = env_obj.reset()
        key = tuple(np.round(state,2))
        if key in Q_table:
            action = int(np.argmax(Q_table[key]))
        else:
            action = env_obj.action_space.sample()
        env_obj.index = int(idx)
        _, _, _, _, info = env_obj.step(action)
        metrics['unproductive'].append(info['unproductive'])
        metrics['satisfaction'].append(info['satisfaction'])
    return {k: np.mean(v) for k,v in metrics.items()}

def evaluate_policy_pg(policy_net, env_obj, n_samples=300):
    idxs = np.random.choice(len(env_obj.df), size=n_samples, replace=False)
    metrics = {'unproductive': [], 'satisfaction': []}
    for idx in idxs:
        env_obj.index = int(idx)
        state, _ = env_obj.reset()
        state_t = torch.tensor(state, dtype=torch.float32)
        with torch.no_grad():
            probs = policy_net(state_t).numpy()
        action = int(np.argmax(probs))
        env_obj.index = int(idx)
        _, _, _, _, info = env_obj.step(action)
        metrics['unproductive'].append(info['unproductive'])
        metrics['satisfaction'].append(info['satisfaction'])
    return {k: np.mean(v) for k,v in metrics.items()}

print('Evaluating policies on held-out samples...')
eval_q = evaluate_policy_q(Q, env, n_samples=300)
eval_pg = evaluate_policy_pg(policy, env, n_samples=300)
print('Q eval:', eval_q)
print('PG eval:', eval_pg)
